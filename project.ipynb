{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 243,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import opendatasets as od\n",
    "import time\n",
    "import datetime\n",
    "from torchsummary import summary\n",
    "import torch.nn as nn\n",
    "\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping, found downloaded files in \"./new-york-city-taxi-fare-prediction\" (use force=True to force download)\n"
     ]
    }
   ],
   "source": [
    "dataset_url = 'https://www.kaggle.com/competitions/new-york-city-taxi-fare-prediction'\n",
    "od.download(dataset_url)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "outputs": [
    {
     "data": {
      "text/plain": "'2010-04-25 03:59:42 UTC'"
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('./NYCTaxiFares_small.csv')\n",
    "df['pickup_datetime'].max()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "outputs": [],
   "source": [
    "## Running this cell is really fucking slow, but it works.\n",
    "df['pickup_datetime'] = pd.to_datetime(df['pickup_datetime'])\n",
    "\n",
    "def havardsine_distance(lat1, long1, lat2, long2):\n",
    "    r = 6371\n",
    "\n",
    "    phi1 = np.radians(lat1)\n",
    "    phi2 = np.radians(lat2)\n",
    "\n",
    "    delta_phi = np.radians(lat2 - lat1)\n",
    "    delta_lambda = np.radians(long2 - long1)\n",
    "    a = np.sin(delta_phi / 2.0) ** 2 + np.cos(phi1) * np.cos(phi2) * np.sin(delta_lambda / 2.0) ** 2\n",
    "    c = 2 * np.arctan2(np.sqrt(a), np.sqrt(1 - a))\n",
    "    d = (r * c) # convert to kilometer\n",
    "    return d\n",
    "\n",
    "df['dist_km'] = df.apply(lambda row: havardsine_distance(row.pickup_latitude, row.pickup_longitude, row.dropoff_latitude, row.dropoff_longitude), axis=1)\n",
    "\n",
    "airport_cords = [\n",
    "    [40.691547, -74.180202], # NEWARK\n",
    "    [40.773281, -73.869845], # LAGUARDIA\n",
    "]\n",
    "\n",
    "# When I wrote this code, only God and I knew what was going on. Today, only God knows.\n",
    "def close_to_airport(values):\n",
    "    pickup_latitude = values['pickup_latitude']\n",
    "    pickup_longitude = values['pickup_longitude']\n",
    "    dropoff_latitude = values['dropoff_latitude']\n",
    "    dropoff_longitude = values['dropoff_longitude']\n",
    "\n",
    "    # JFK is special, it needs a bigger radius\n",
    "    jfkpickup = havardsine_distance(pickup_latitude, pickup_longitude, 40.645042, -73.786928) <= 1.00\n",
    "    jfkdropoff = havardsine_distance(dropoff_latitude, dropoff_longitude, 40.645042, -73.786928) <= 1.00\n",
    "\n",
    "    # Newark is a small airport but their parking log is BIG.\n",
    "    newarkpickup = havardsine_distance(pickup_latitude, pickup_longitude, 40.691547, -74.180202) <= 1.00\n",
    "    newarkdropoff = havardsine_distance(dropoff_latitude, dropoff_longitude, 40.691547, -74.180202) <= 1.00\n",
    "\n",
    "    # Third largest, it's an abstract airport with a stretched out U shaped parking place\n",
    "    # LaGuardia\n",
    "    laguardiapickup = havardsine_distance(pickup_latitude, pickup_longitude, 40.773855, -73.871712) <= 0.50\n",
    "    laguardiadropoff = havardsine_distance(dropoff_latitude, dropoff_longitude, 40.773855, -73.871712) <= 0.50\n",
    "\n",
    "    return 1 if jfkpickup or jfkdropoff or newarkpickup or newarkdropoff or laguardiapickup or laguardiadropoff else 0\n",
    "\n",
    "df['is_by_airport'] = df.apply(close_to_airport, axis=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "outputs": [],
   "source": [
    "my_time = df['pickup_datetime'][0]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "outputs": [],
   "source": [
    "df['EDTdate'] = df['pickup_datetime'] - pd.Timedelta(hours=4)\n",
    "df['Hour'] = df['EDTdate'].dt.hour\n",
    "df['AMorPM'] = np.where(df['Hour'] < 12, 0, 1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['04/19/10' '04/17/10' '04/11/10' '04/16/10' '04/22/10' '04/23/10'\n",
      " '04/15/10' '04/20/10' '04/21/10' '04/14/10' '04/13/10' '04/12/10'\n",
      " '04/24/10' '04/18/10']\n"
     ]
    }
   ],
   "source": [
    "df['Weekday'] = pd.to_numeric(df['EDTdate'].dt.strftime(\"%w\")) # an int between 0 - 6, representing the weekdays\n",
    "df['Month'] = pd.to_numeric(df['EDTdate'].dt.strftime(\"%m\")) # an int between 0 - 6, representing the weekdays\n",
    "\n",
    "#Formatting this correctly, we can use the dates to get the historical weather data for the dates\n",
    "df['FullDate'] = df['EDTdate'].dt.strftime('%x')\n",
    "#print(df['FullDate'].min())\n",
    "#print(df['FullDate'].max())\n",
    "print(df['FullDate'].unique())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "outputs": [
    {
     "data": {
      "text/plain": "'\\napi_key_openweather = \"661b8c5b6e790b9cf55403d0af1cebe8\"\\nnew_york_middle_lat = \"\"\\nnew_york_middle_long = \"\"\\nunique = np.sort(df[\\'FullDate\\'].unique())\\nstart = time.mktime(datetime.datetime.strptime(unique[0], \"%m/%d/%y\").timetuple())\\nend = time.mktime(datetime.datetime.strptime(unique[len(unique) - 1], \"%m/%d/%y\").timetuple())\\nurl = f\"https://history.openweathermap.org/data/2.5/history/city?lat={new_york_middle_lat}&lon={new_york_middle_long}&type=hour&start={start}&end={end}&appid={api_key_openweather}\"\\n\\nprint(url)\\n'"
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "api_key_openweather = \"661b8c5b6e790b9cf55403d0af1cebe8\"\n",
    "new_york_middle_lat = \"\"\n",
    "new_york_middle_long = \"\"\n",
    "unique = np.sort(df['FullDate'].unique())\n",
    "start = time.mktime(datetime.datetime.strptime(unique[0], \"%m/%d/%y\").timetuple())\n",
    "end = time.mktime(datetime.datetime.strptime(unique[len(unique) - 1], \"%m/%d/%y\").timetuple())\n",
    "url = f\"https://history.openweathermap.org/data/2.5/history/city?lat={new_york_middle_lat}&lon={new_york_middle_long}&type=hour&start={start}&end={end}&appid={api_key_openweather}\"\n",
    "\n",
    "print(url)\n",
    "'''\n",
    "# Historical weather data, not working"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "outputs": [],
   "source": [
    "# lets have some weather-y fuuuuuuuuuuuun-fuuuuuun-function\n",
    "# see this for doc https://www1.ncdc.noaa.gov/pub/data/cdo/documentation/GHCND_documentation.pdf\n",
    "weather_data = pd.read_csv('nyc-weather-data.csv')\n",
    "weather_labels = ['SNWD', 'SNOW', 'AWND'] #snow depth, did it snow\n",
    "for index, item in enumerate(weather_labels):\n",
    "    weather_data.loc[weather_data[item] <= -9999, item] = 0 # -9999 means that there is no data\n",
    "\n",
    "# df['dist_km'] = df.apply(lambda row: havardsine_distance(row.pickup_latitude, row.pickup_longitude, row.dropoff_latitude, row.dropoff_longitude), axis=1)\n",
    "\n",
    "weather_data['DATE'] = weather_data.apply(lambda row: datetime.datetime.strptime(str(row.DATE), '%Y%m%d').strftime('%m/%d/%y'), axis=1)\n",
    "weather_data = weather_data[['DATE','PRCP','SNWD','SNOW','TMAX','TMIN','AWND']]\n",
    "\n",
    "weather_data['TMIN'] = (weather_data['TMIN'] / 10) # the API returns degrees in an old format. We therefore convert the value to a tenth of its own form. Check the docu for this.\n",
    "weather_data['TMAX'] = (weather_data['TMAX'] / 10)\n",
    "weather_data['PRCP'] = weather_data['PRCP'] / 10\n",
    "\n",
    "weather_data['AWND'] = weather_data['AWND'] / 2.237 # meters per second\n",
    "\n",
    "df.rename(columns={'FullDate':'DATE'}, inplace=True)\n",
    "\n",
    "merged_df = pd.merge(df, weather_data, how=\"left\",on=\"DATE\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "outputs": [],
   "source": [
    "network_data = merged_df[['pickup_latitude', 'pickup_longitude', 'dropoff_latitude', 'dropoff_longitude', 'dist_km', 'passenger_count', 'is_by_airport', 'Hour', 'AMorPM', 'Weekday', 'Month', 'PRCP', 'SNWD', 'SNOW', 'TMAX', 'TMIN', 'AWND', 'fare_amount']]\n",
    "\n",
    "prediction = network_data['fare_amount']\n",
    "\n",
    "network_data.drop('fare_amount', axis=1)\n",
    "\n",
    "X = network_data.values\n",
    "\n",
    "y = prediction.values.reshape(-1,1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "outputs": [],
   "source": [
    "train_X, test_X, train_y, test_y = train_test_split(X, y, test_size=0.1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "outputs": [],
   "source": [
    "X = torch.tensor(train_X, dtype=torch.float)\n",
    "y = torch.tensor(train_y, dtype=torch.int64)\n",
    "test_X_tensor = torch.Tensor(test_X)\n",
    "test_y_tensor = torch.Tensor(test_y)\n",
    "\n",
    "train_ds = torch.utils.data.TensorDataset(X, y)\n",
    "test_ds = torch.utils.data.TensorDataset(test_X_tensor, test_y_tensor)\n",
    "\n",
    "train_dl = torch.utils.data.DataLoader(train_ds, batch_size=128, shuffle=True)\n",
    "test_dl = torch.utils.data.DataLoader(test_ds, batch_size=128, shuffle=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Linear-1                   [-1, 42]             798\n",
      "              ReLU-2                   [-1, 42]               0\n",
      "            Linear-3                   [-1, 42]           1,806\n",
      "              ReLU-4                   [-1, 42]               0\n",
      "            Linear-5                   [-1, 42]           1,806\n",
      "              ReLU-6                   [-1, 42]               0\n",
      "            Linear-7                    [-1, 1]              43\n",
      "================================================================\n",
      "Total params: 4,453\n",
      "Trainable params: 4,453\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.00\n",
      "Params size (MB): 0.02\n",
      "Estimated Total Size (MB): 0.02\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "class NYCTaxiPredictor(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(18, 42),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(42, 42),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(42, 42),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(42, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "\n",
    "model = NYCTaxiPredictor()\n",
    "summary(model, tuple([18]))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.1477],\n",
      "        [-0.0771],\n",
      "        [ 0.2593],\n",
      "        [ 0.1094],\n",
      "        [-0.0161],\n",
      "        [ 0.0724],\n",
      "        [ 0.2471],\n",
      "        [ 0.0492],\n",
      "        [ 0.1683],\n",
      "        [-0.0482],\n",
      "        [ 0.1937],\n",
      "        [ 0.0463],\n",
      "        [-0.0663],\n",
      "        [ 0.0932],\n",
      "        [ 0.1123],\n",
      "        [ 0.1103],\n",
      "        [-0.1303],\n",
      "        [ 0.1513],\n",
      "        [-0.0039],\n",
      "        [ 0.1140],\n",
      "        [ 0.1601],\n",
      "        [ 0.0058],\n",
      "        [ 0.0665],\n",
      "        [-0.0589],\n",
      "        [ 0.0178],\n",
      "        [-0.0392],\n",
      "        [ 0.0477],\n",
      "        [ 0.1288],\n",
      "        [ 0.1888],\n",
      "        [ 0.0523],\n",
      "        [-0.0176],\n",
      "        [ 0.1728],\n",
      "        [ 0.1143],\n",
      "        [ 0.2114],\n",
      "        [ 0.1922],\n",
      "        [ 0.1508],\n",
      "        [-0.0668],\n",
      "        [-0.0604],\n",
      "        [ 0.0441],\n",
      "        [ 0.1177],\n",
      "        [ 0.0584],\n",
      "        [ 0.0151],\n",
      "        [-0.0011],\n",
      "        [ 0.0564],\n",
      "        [ 0.0381],\n",
      "        [ 0.0585],\n",
      "        [-0.0472],\n",
      "        [ 0.0217],\n",
      "        [-0.0592],\n",
      "        [ 0.0692],\n",
      "        [-0.0021],\n",
      "        [ 0.1466],\n",
      "        [-0.0507],\n",
      "        [-0.0028],\n",
      "        [ 0.0767],\n",
      "        [-0.0199],\n",
      "        [-0.0148],\n",
      "        [ 0.1269],\n",
      "        [ 0.1512],\n",
      "        [ 0.1846],\n",
      "        [ 0.0314],\n",
      "        [-0.0127],\n",
      "        [ 0.1201],\n",
      "        [-0.0137],\n",
      "        [ 0.0120],\n",
      "        [ 0.1660],\n",
      "        [ 0.1402],\n",
      "        [-0.0844],\n",
      "        [ 0.1931],\n",
      "        [ 0.1546],\n",
      "        [-0.0419],\n",
      "        [ 0.0838],\n",
      "        [-0.0148],\n",
      "        [ 0.0279],\n",
      "        [ 0.1379],\n",
      "        [ 0.1743],\n",
      "        [-0.0601],\n",
      "        [ 0.2197],\n",
      "        [ 0.0203],\n",
      "        [ 0.0167],\n",
      "        [-0.0781],\n",
      "        [ 0.1192],\n",
      "        [-0.0077],\n",
      "        [ 0.0876],\n",
      "        [-0.0560],\n",
      "        [ 0.0112],\n",
      "        [-0.0085],\n",
      "        [ 0.0057],\n",
      "        [ 0.0061],\n",
      "        [ 0.1333],\n",
      "        [ 0.1424],\n",
      "        [ 0.1707],\n",
      "        [ 0.1187],\n",
      "        [ 0.1513],\n",
      "        [-0.0123],\n",
      "        [-0.0851],\n",
      "        [ 0.0863],\n",
      "        [ 0.1098],\n",
      "        [ 0.0435],\n",
      "        [ 0.2878],\n",
      "        [-0.0736],\n",
      "        [ 0.0170],\n",
      "        [ 0.0759],\n",
      "        [ 0.0442],\n",
      "        [-0.0323],\n",
      "        [-0.0358],\n",
      "        [ 0.0445],\n",
      "        [-0.0003],\n",
      "        [ 0.1331],\n",
      "        [ 0.0417],\n",
      "        [-0.0173],\n",
      "        [-0.0398],\n",
      "        [-0.0303],\n",
      "        [ 0.1391],\n",
      "        [ 0.2807],\n",
      "        [ 0.1446],\n",
      "        [ 0.0890],\n",
      "        [-0.0078],\n",
      "        [ 0.1698],\n",
      "        [ 0.1932],\n",
      "        [-0.0204],\n",
      "        [ 0.0801],\n",
      "        [ 0.0513],\n",
      "        [-0.0459],\n",
      "        [ 0.1706],\n",
      "        [ 0.1102],\n",
      "        [ 0.1983],\n",
      "        [-0.0309]], grad_fn=<AddmmBackward0>) tensor([[11],\n",
      "        [ 6],\n",
      "        [ 7],\n",
      "        [10],\n",
      "        [13],\n",
      "        [ 7],\n",
      "        [ 4],\n",
      "        [13],\n",
      "        [12],\n",
      "        [27],\n",
      "        [ 3],\n",
      "        [ 8],\n",
      "        [11],\n",
      "        [13],\n",
      "        [ 6],\n",
      "        [ 8],\n",
      "        [17],\n",
      "        [ 9],\n",
      "        [ 8],\n",
      "        [ 4],\n",
      "        [ 6],\n",
      "        [13],\n",
      "        [ 8],\n",
      "        [ 6],\n",
      "        [ 8],\n",
      "        [12],\n",
      "        [ 6],\n",
      "        [ 4],\n",
      "        [ 5],\n",
      "        [ 3],\n",
      "        [15],\n",
      "        [ 4],\n",
      "        [ 4],\n",
      "        [ 5],\n",
      "        [15],\n",
      "        [14],\n",
      "        [10],\n",
      "        [10],\n",
      "        [ 4],\n",
      "        [12],\n",
      "        [ 7],\n",
      "        [ 6],\n",
      "        [11],\n",
      "        [ 4],\n",
      "        [ 6],\n",
      "        [ 3],\n",
      "        [ 5],\n",
      "        [30],\n",
      "        [11],\n",
      "        [ 4],\n",
      "        [ 6],\n",
      "        [ 5],\n",
      "        [11],\n",
      "        [ 4],\n",
      "        [ 7],\n",
      "        [ 7],\n",
      "        [ 9],\n",
      "        [20],\n",
      "        [ 5],\n",
      "        [ 3],\n",
      "        [ 8],\n",
      "        [ 3],\n",
      "        [ 6],\n",
      "        [29],\n",
      "        [ 8],\n",
      "        [ 8],\n",
      "        [ 6],\n",
      "        [21],\n",
      "        [10],\n",
      "        [ 7],\n",
      "        [13],\n",
      "        [10],\n",
      "        [ 6],\n",
      "        [ 3],\n",
      "        [ 6],\n",
      "        [10],\n",
      "        [17],\n",
      "        [ 6],\n",
      "        [ 4],\n",
      "        [ 4],\n",
      "        [34],\n",
      "        [ 8],\n",
      "        [12],\n",
      "        [ 7],\n",
      "        [22],\n",
      "        [ 7],\n",
      "        [12],\n",
      "        [ 6],\n",
      "        [29],\n",
      "        [ 7],\n",
      "        [ 8],\n",
      "        [ 4],\n",
      "        [ 3],\n",
      "        [14],\n",
      "        [ 7],\n",
      "        [18],\n",
      "        [13],\n",
      "        [ 6],\n",
      "        [ 4],\n",
      "        [ 4],\n",
      "        [12],\n",
      "        [36],\n",
      "        [13],\n",
      "        [ 3],\n",
      "        [ 8],\n",
      "        [ 5],\n",
      "        [ 8],\n",
      "        [ 4],\n",
      "        [ 3],\n",
      "        [ 4],\n",
      "        [ 8],\n",
      "        [ 8],\n",
      "        [ 4],\n",
      "        [ 9],\n",
      "        [ 5],\n",
      "        [ 6],\n",
      "        [ 4],\n",
      "        [ 7],\n",
      "        [ 9],\n",
      "        [10],\n",
      "        [24],\n",
      "        [ 8],\n",
      "        [19],\n",
      "        [ 7],\n",
      "        [ 8],\n",
      "        [10],\n",
      "        [ 5],\n",
      "        [10]])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Found dtype Long but expected Float",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[0;32mIn [378], line 10\u001B[0m\n\u001B[1;32m      8\u001B[0m     \u001B[38;5;28mprint\u001B[39m(output, y)\n\u001B[1;32m      9\u001B[0m     loss \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39msqrt(criterion(output, y))\n\u001B[0;32m---> 10\u001B[0m     loss\u001B[38;5;241m.\u001B[39mbackward()\n\u001B[1;32m     11\u001B[0m     optimizer\u001B[38;5;241m.\u001B[39mstep()\n\u001B[1;32m     12\u001B[0m \u001B[38;5;28mprint\u001B[39m(loss\u001B[38;5;241m.\u001B[39mdata)\n",
      "File \u001B[0;32m~/PycharmProjects/pythonProject/venv/lib/python3.8/site-packages/torch/_tensor.py:396\u001B[0m, in \u001B[0;36mTensor.backward\u001B[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001B[0m\n\u001B[1;32m    387\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m has_torch_function_unary(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m    388\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(\n\u001B[1;32m    389\u001B[0m         Tensor\u001B[38;5;241m.\u001B[39mbackward,\n\u001B[1;32m    390\u001B[0m         (\u001B[38;5;28mself\u001B[39m,),\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    394\u001B[0m         create_graph\u001B[38;5;241m=\u001B[39mcreate_graph,\n\u001B[1;32m    395\u001B[0m         inputs\u001B[38;5;241m=\u001B[39minputs)\n\u001B[0;32m--> 396\u001B[0m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mautograd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgradient\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/pythonProject/venv/lib/python3.8/site-packages/torch/autograd/__init__.py:173\u001B[0m, in \u001B[0;36mbackward\u001B[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[0m\n\u001B[1;32m    168\u001B[0m     retain_graph \u001B[38;5;241m=\u001B[39m create_graph\n\u001B[1;32m    170\u001B[0m \u001B[38;5;66;03m# The reason we repeat same the comment below is that\u001B[39;00m\n\u001B[1;32m    171\u001B[0m \u001B[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001B[39;00m\n\u001B[1;32m    172\u001B[0m \u001B[38;5;66;03m# calls in the traceback and some print out the last line\u001B[39;00m\n\u001B[0;32m--> 173\u001B[0m \u001B[43mVariable\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_execution_engine\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun_backward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001B[39;49;00m\n\u001B[1;32m    174\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtensors\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgrad_tensors_\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    175\u001B[0m \u001B[43m    \u001B[49m\u001B[43mallow_unreachable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maccumulate_grad\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mRuntimeError\u001B[0m: Found dtype Long but expected Float"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.MSELoss()\n",
    "for epochs in range(500):\n",
    "    for data in train_dl:\n",
    "        X, y = data\n",
    "        optimizer.zero_grad()\n",
    "        output = model(X)\n",
    "        print(output, y)\n",
    "        loss = torch.sqrt(criterion(output, y))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(loss.data)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}