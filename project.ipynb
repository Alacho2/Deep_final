{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import opendatasets as od\n",
    "import time\n",
    "import math\n",
    "import datetime\n",
    "from torchsummary import summary\n",
    "import torch.nn as nn\n",
    "pd.options.mode.chained_assignment = None\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "tqdm.pandas()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping, found downloaded files in \"./new-york-city-taxi-fare-prediction\" (use force=True to force download)\n"
     ]
    }
   ],
   "source": [
    "dataset_url = 'https://www.kaggle.com/competitions/new-york-city-taxi-fare-prediction'\n",
    "od.download(dataset_url)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "df = pd.read_csv('./new-york-city-taxi-fare-prediction/train.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "date_cap = datetime.datetime(2015, 12, 31)\n",
    "date_cap < datetime.datetime.now()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def ToTimeStamp(row):\n",
    "    return pd.Timestamp(row)\n",
    "df['pickup_datetime'] = pd.to_datetime(df['pickup_datetime'])\n",
    "#df['month_formatted'] = df['pickup_datetime'].progress_apply(lambda row: pd.Period(row, freq='D'))\n",
    "\n",
    "#2010-04-18\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "0           2009-06-15 17:26:21 UTC\n1           2010-01-05 16:52:16 UTC\n2           2011-08-18 00:35:00 UTC\n3           2012-04-21 04:30:42 UTC\n4           2010-03-09 07:51:00 UTC\n                     ...           \n55423851    2014-03-15 03:28:00 UTC\n55423852    2009-03-24 20:46:20 UTC\n55423853    2011-04-02 22:04:24 UTC\n55423854    2011-10-26 05:57:51 UTC\n55423855    2014-12-12 11:33:00 UTC\nName: pickup_datetime, Length: 55423856, dtype: object"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['pickup_datetime']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df=df[~(df['month_formatted']<'2015-12-31')]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2015-06-30 23:59:54 UTC\n",
      "2009-01-01 00:00:27 UTC\n"
     ]
    }
   ],
   "source": [
    "print(df['pickup_datetime'].max())\n",
    "print(df['pickup_datetime'].min())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "outputs": [
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "outputs": [],
   "source": [
    "## Running this cell is really fucking slow, but it works.\n",
    "df['pickup_datetime'] = pd.to_datetime(df['pickup_datetime'])\n",
    "\n",
    "def havardsine_distance(lat1, long1, lat2, long2):\n",
    "    r = 6371\n",
    "\n",
    "    phi1 = np.radians(lat1)\n",
    "    phi2 = np.radians(lat2)\n",
    "\n",
    "    delta_phi = np.radians(lat2 - lat1)\n",
    "    delta_lambda = np.radians(long2 - long1)\n",
    "    a = np.sin(delta_phi / 2.0) ** 2 + np.cos(phi1) * np.cos(phi2) * np.sin(delta_lambda / 2.0) ** 2\n",
    "    c = 2 * np.arctan2(np.sqrt(a), np.sqrt(1 - a))\n",
    "    d = (r * c) # convert to kilometer\n",
    "    return d\n",
    "\n",
    "df['dist_km'] = df.apply(lambda row: havardsine_distance(row.pickup_latitude, row.pickup_longitude, row.dropoff_latitude, row.dropoff_longitude), axis=1)\n",
    "\n",
    "airport_cords = [\n",
    "    [40.691547, -74.180202], # NEWARK\n",
    "    [40.773281, -73.869845], # LAGUARDIA\n",
    "]\n",
    "\n",
    "# When I wrote this code, only God and I knew what was going on. Today, only God knows.\n",
    "def close_to_airport(values):\n",
    "    pickup_latitude = values['pickup_latitude']\n",
    "    pickup_longitude = values['pickup_longitude']\n",
    "    dropoff_latitude = values['dropoff_latitude']\n",
    "    dropoff_longitude = values['dropoff_longitude']\n",
    "\n",
    "    # JFK is special, it needs a bigger radius\n",
    "    jfkpickup = havardsine_distance(pickup_latitude, pickup_longitude, 40.645042, -73.786928) <= 1.00\n",
    "    jfkdropoff = havardsine_distance(dropoff_latitude, dropoff_longitude, 40.645042, -73.786928) <= 1.00\n",
    "\n",
    "    # Newark is a small airport but their parking log is BIG.\n",
    "    newarkpickup = havardsine_distance(pickup_latitude, pickup_longitude, 40.691547, -74.180202) <= 1.00\n",
    "    newarkdropoff = havardsine_distance(dropoff_latitude, dropoff_longitude, 40.691547, -74.180202) <= 1.00\n",
    "\n",
    "    # Third largest, it's an abstract airport with a stretched out U shaped parking place\n",
    "    # LaGuardia\n",
    "    laguardiapickup = havardsine_distance(pickup_latitude, pickup_longitude, 40.773855, -73.871712) <= 0.50\n",
    "    laguardiadropoff = havardsine_distance(dropoff_latitude, dropoff_longitude, 40.773855, -73.871712) <= 0.50\n",
    "\n",
    "    return 1 if jfkpickup or jfkdropoff or newarkpickup or newarkdropoff or laguardiapickup or laguardiadropoff else 0\n",
    "\n",
    "df['is_by_airport'] = df.apply(close_to_airport, axis=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "outputs": [],
   "source": [
    "my_time = df['pickup_datetime'][0]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "outputs": [],
   "source": [
    "df['EDTdate'] = df['pickup_datetime'] - pd.Timedelta(hours=4)\n",
    "df['Hour'] = df['EDTdate'].dt.hour\n",
    "df['AMorPM'] = np.where(df['Hour'] < 12, 0, 1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['04/19/10' '04/17/10' '04/11/10' '04/16/10' '04/22/10' '04/23/10'\n",
      " '04/15/10' '04/20/10' '04/21/10' '04/14/10' '04/13/10' '04/12/10'\n",
      " '04/24/10' '04/18/10']\n"
     ]
    }
   ],
   "source": [
    "df['Weekday'] = pd.to_numeric(df['EDTdate'].dt.strftime(\"%w\")) # an int between 0 - 6, representing the weekdays\n",
    "df['Month'] = pd.to_numeric(df['EDTdate'].dt.strftime(\"%m\")) # an int between 0 - 6, representing the weekdays\n",
    "\n",
    "#Formatting this correctly, we can use the dates to get the historical weather data for the dates\n",
    "df['FullDate'] = df['EDTdate'].dt.strftime('%x')\n",
    "#print(df['FullDate'].min())\n",
    "#print(df['FullDate'].max())\n",
    "print(df['FullDate'].unique())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "outputs": [
    {
     "data": {
      "text/plain": "'\\napi_key_openweather = \"661b8c5b6e790b9cf55403d0af1cebe8\"\\nnew_york_middle_lat = \"\"\\nnew_york_middle_long = \"\"\\nunique = np.sort(df[\\'FullDate\\'].unique())\\nstart = time.mktime(datetime.datetime.strptime(unique[0], \"%m/%d/%y\").timetuple())\\nend = time.mktime(datetime.datetime.strptime(unique[len(unique) - 1], \"%m/%d/%y\").timetuple())\\nurl = f\"https://history.openweathermap.org/data/2.5/history/city?lat={new_york_middle_lat}&lon={new_york_middle_long}&type=hour&start={start}&end={end}&appid={api_key_openweather}\"\\n\\nprint(url)\\n'"
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "api_key_openweather = \"661b8c5b6e790b9cf55403d0af1cebe8\"\n",
    "new_york_middle_lat = \"\"\n",
    "new_york_middle_long = \"\"\n",
    "unique = np.sort(df['FullDate'].unique())\n",
    "start = time.mktime(datetime.datetime.strptime(unique[0], \"%m/%d/%y\").timetuple())\n",
    "end = time.mktime(datetime.datetime.strptime(unique[len(unique) - 1], \"%m/%d/%y\").timetuple())\n",
    "url = f\"https://history.openweathermap.org/data/2.5/history/city?lat={new_york_middle_lat}&lon={new_york_middle_long}&type=hour&start={start}&end={end}&appid={api_key_openweather}\"\n",
    "\n",
    "print(url)\n",
    "'''\n",
    "# Historical weather data, not working"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "outputs": [
    {
     "data": {
      "text/plain": "'12/31/14'"
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lets have some weather-y fuuuuuuuuuuuun-fuuuuuun-function\n",
    "# see this for doc https://www1.ncdc.noaa.gov/pub/data/cdo/documentation/GHCND_documentation.pdf\n",
    "weather_data = pd.read_csv('nyc-weather-data.csv')\n",
    "weather_labels = ['SNWD', 'SNOW', 'AWND'] #snow depth, did it snow\n",
    "for index, item in enumerate(weather_labels):\n",
    "    weather_data.loc[weather_data[item] <= -9999, item] = 0 # -9999 means that there is no data\n",
    "\n",
    "# df['dist_km'] = df.apply(lambda row: havardsine_distance(row.pickup_latitude, row.pickup_longitude, row.dropoff_latitude, row.dropoff_longitude), axis=1)\n",
    "\n",
    "weather_data['DATE'] = weather_data.apply(lambda row: datetime.datetime.strptime(str(row.DATE), '%Y%m%d').strftime('%m/%d/%y'), axis=1)\n",
    "weather_data = weather_data[['DATE','PRCP','SNWD','SNOW','TMAX','TMIN','AWND']]\n",
    "\n",
    "weather_data['TMIN'] = (weather_data['TMIN'] / 10) # the API returns degrees in an old format. We therefore convert the value to a tenth of its own form. Check the docu for this.\n",
    "weather_data['TMAX'] = (weather_data['TMAX'] / 10)\n",
    "weather_data['PRCP'] = weather_data['PRCP'] / 10\n",
    "\n",
    "weather_data['AWND'] = weather_data['AWND'] / 2.237 # meters per second\n",
    "\n",
    "weather_data['DATE'].max()\n",
    "\n",
    "#df.rename(columns={'FullDate':'DATE'}, inplace=True)\n",
    "\n",
    "#merged_df = pd.merge(df, weather_data, how=\"left\",on=\"DATE\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(120000,)\n"
     ]
    }
   ],
   "source": [
    "network_data = merged_df[['pickup_latitude', 'pickup_longitude', 'dropoff_latitude', 'dropoff_longitude', 'dist_km', 'passenger_count', 'is_by_airport', 'Hour', 'AMorPM', 'Weekday', 'PRCP', 'TMAX', 'TMIN', 'AWND', 'fare_amount']]\n",
    "\n",
    "network_data['pickup_longitude'] = network_data.apply(lambda row: abs(row.pickup_longitude), axis=1)\n",
    "\n",
    "network_data['dropoff_longitude'] = network_data.apply(lambda row: abs(row.pickup_longitude), axis=1)\n",
    "\n",
    "network_data.iloc[:,0:-1] = network_data.iloc[:,0:-1].apply(lambda x: (x-x.mean())/ x.std(), axis=0)\n",
    "\n",
    "prediction = network_data['fare_amount']\n",
    "\n",
    "network_data.drop('fare_amount', axis=1, inplace=True)\n",
    "X = network_data.values\n",
    "\n",
    "y = prediction.values"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "outputs": [],
   "source": [
    "train_X, test_X, train_y, test_y = train_test_split(X, y, test_size=0.1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "outputs": [],
   "source": [
    "X = torch.tensor(train_X, dtype=torch.float32)\n",
    "y = torch.tensor(train_y, dtype=torch.long)\n",
    "test_X_tensor = torch.Tensor(test_X)\n",
    "test_y_tensor = torch.LongTensor(test_y)\n",
    "\n",
    "train_ds = torch.utils.data.TensorDataset(X, y)\n",
    "test_ds = torch.utils.data.TensorDataset(test_X_tensor, test_y_tensor)\n",
    "\n",
    "train_dl = torch.utils.data.DataLoader(train_ds, batch_size=128, shuffle=True)\n",
    "test_dl = torch.utils.data.DataLoader(test_ds, batch_size=128, shuffle=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Linear-1                   [-1, 64]             960\n",
      "              ReLU-2                   [-1, 64]               0\n",
      "            Linear-3                   [-1, 64]           4,160\n",
      "       BatchNorm1d-4                   [-1, 64]             128\n",
      "           Dropout-5                   [-1, 64]               0\n",
      "              ReLU-6                   [-1, 64]               0\n",
      "            Linear-7                  [-1, 128]           8,320\n",
      "       BatchNorm1d-8                  [-1, 128]             256\n",
      "           Dropout-9                  [-1, 128]               0\n",
      "             ReLU-10                  [-1, 128]               0\n",
      "           Linear-11                  [-1, 128]          16,512\n",
      "       LogSoftmax-12                  [-1, 128]               0\n",
      "================================================================\n",
      "Total params: 30,336\n",
      "Trainable params: 30,336\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.01\n",
      "Params size (MB): 0.12\n",
      "Estimated Total Size (MB): 0.12\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "class NYCTaxiPredictor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NYCTaxiPredictor, self).__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(14, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.Dropout(0.6),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.Dropout(0.6),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.LogSoftmax(dim=-1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "\n",
    "model = NYCTaxiPredictor()\n",
    "summary(model, tuple([14]))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.1231) 0\n",
      "tensor(1.9682) 1\n",
      "tensor(1.9641) 2\n",
      "tensor(2.1875) 3\n",
      "tensor(2.0390) 4\n",
      "tensor(2.1349) 5\n",
      "tensor(2.1541) 6\n",
      "tensor(1.8245) 7\n",
      "tensor(2.0170) 8\n",
      "tensor(2.0522) 9\n",
      "tensor(1.9900) 10\n",
      "tensor(1.8851) 11\n",
      "tensor(1.8808) 12\n",
      "tensor(2.1794) 13\n",
      "tensor(2.0594) 14\n",
      "tensor(2.0441) 15\n",
      "tensor(2.0263) 16\n",
      "tensor(1.8839) 17\n",
      "tensor(1.9900) 18\n",
      "tensor(2.0724) 19\n",
      "tensor(1.7681) 20\n",
      "tensor(1.9962) 21\n",
      "tensor(2.1896) 22\n",
      "tensor(1.8728) 23\n",
      "tensor(2.0756) 24\n",
      "tensor(1.9152) 25\n",
      "tensor(1.9879) 26\n",
      "tensor(1.8857) 27\n",
      "tensor(1.8706) 28\n",
      "tensor(1.8354) 29\n",
      "tensor(1.8824) 30\n",
      "tensor(2.1126) 31\n",
      "tensor(2.1036) 32\n",
      "tensor(2.0349) 33\n",
      "tensor(1.9256) 34\n",
      "tensor(1.9295) 35\n",
      "tensor(1.6889) 36\n",
      "tensor(2.1701) 37\n",
      "tensor(1.9238) 38\n",
      "tensor(1.7796) 39\n",
      "tensor(1.7728) 40\n",
      "tensor(2.0054) 41\n",
      "tensor(1.9527) 42\n",
      "tensor(1.8442) 43\n",
      "tensor(1.9331) 44\n",
      "tensor(2.2847) 45\n",
      "tensor(2.0903) 46\n",
      "tensor(1.8294) 47\n",
      "tensor(1.9824) 48\n",
      "tensor(1.8595) 49\n",
      "tensor(1.9283) 50\n",
      "tensor(1.9118) 51\n",
      "tensor(1.8806) 52\n",
      "tensor(2.3116) 53\n",
      "tensor(1.9915) 54\n",
      "tensor(1.8433) 55\n",
      "tensor(1.9727) 56\n",
      "tensor(1.8853) 57\n",
      "tensor(1.9879) 58\n",
      "tensor(1.6828) 59\n",
      "tensor(1.7328) 60\n",
      "tensor(1.7891) 61\n",
      "tensor(1.9706) 62\n",
      "tensor(2.0187) 63\n",
      "tensor(1.7224) 64\n",
      "tensor(1.8520) 65\n",
      "tensor(2.0801) 66\n",
      "tensor(1.8635) 67\n",
      "tensor(1.9666) 68\n",
      "tensor(1.6859) 69\n",
      "tensor(1.8348) 70\n",
      "tensor(1.8628) 71\n",
      "tensor(1.8671) 72\n",
      "tensor(1.7911) 73\n",
      "tensor(2.0716) 74\n",
      "tensor(1.9004) 75\n",
      "tensor(1.9054) 76\n",
      "tensor(1.9688) 77\n",
      "tensor(1.9010) 78\n",
      "tensor(1.7905) 79\n",
      "tensor(2.2655) 80\n",
      "tensor(1.9359) 81\n",
      "tensor(1.9383) 82\n",
      "tensor(1.9885) 83\n",
      "tensor(1.9762) 84\n",
      "tensor(1.8293) 85\n",
      "tensor(1.9428) 86\n",
      "tensor(1.8302) 87\n",
      "tensor(1.7442) 88\n",
      "tensor(1.8918) 89\n",
      "tensor(1.8848) 90\n",
      "tensor(1.7166) 91\n",
      "tensor(1.9485) 92\n",
      "tensor(1.9926) 93\n",
      "tensor(1.8643) 94\n",
      "tensor(1.9964) 95\n",
      "tensor(1.8034) 96\n",
      "tensor(1.6962) 97\n",
      "tensor(1.8785) 98\n",
      "tensor(1.7175) 99\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=0.0001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "loss_values = []\n",
    "for epochs in range(100):\n",
    "    for data in train_dl:\n",
    "        X, y = data\n",
    "        optimizer.zero_grad()\n",
    "        output = model.forward(X)\n",
    "        #print(output.shape, y.shape)\n",
    "        loss = nn.functional.nll_loss(output, y)\n",
    "        #loss_values.append(loss.item())\n",
    "        #print(loss.item())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(loss.data, epochs)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 33.2\n"
     ]
    }
   ],
   "source": [
    "total = 0\n",
    "correct = 0\n",
    "with torch.no_grad(): # no need to keep track of the gradient. The backward() is doing it\n",
    "  for data in test_dl:\n",
    "    X, y = data\n",
    "    output = model(X)\n",
    "    for idx, val in enumerate(output):\n",
    "      if torch.argmax(val) == y[idx]:\n",
    "        correct += 1\n",
    "      total += 1\n",
    "  print(\"Accuracy:\", round(correct/total, 3) * 100)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}