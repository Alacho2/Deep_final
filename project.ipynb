{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 177,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import opendatasets as od\n",
    "import time\n",
    "import datetime\n",
    "import torch.nn as nn\n",
    "\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping, found downloaded files in \"./new-york-city-taxi-fare-prediction\" (use force=True to force download)\n"
     ]
    }
   ],
   "source": [
    "dataset_url = 'https://www.kaggle.com/competitions/new-york-city-taxi-fare-prediction'\n",
    "od.download(dataset_url)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "outputs": [
    {
     "data": {
      "text/plain": "'2010-04-25 03:59:42 UTC'"
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('./NYCTaxiFares_small.csv')\n",
    "df['pickup_datetime'].max()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "outputs": [],
   "source": [
    "## Running this cell is really fucking slow, but it works.\n",
    "df['pickup_datetime'] = pd.to_datetime(df['pickup_datetime'])\n",
    "\n",
    "def havardsine_distance(lat1, long1, lat2, long2):\n",
    "    r = 6371\n",
    "\n",
    "    phi1 = np.radians(lat1)\n",
    "    phi2 = np.radians(lat2)\n",
    "\n",
    "    delta_phi = np.radians(lat2 - lat1)\n",
    "    delta_lambda = np.radians(long2 - long1)\n",
    "    a = np.sin(delta_phi / 2.0) ** 2 + np.cos(phi1) * np.cos(phi2) * np.sin(delta_lambda / 2.0) ** 2\n",
    "    c = 2 * np.arctan2(np.sqrt(a), np.sqrt(1 - a))\n",
    "    d = (r * c) # convert to kilometer\n",
    "    return d\n",
    "\n",
    "df['dist_km'] = df.apply(lambda row: havardsine_distance(row.pickup_latitude, row.pickup_longitude, row.dropoff_latitude, row.dropoff_longitude), axis=1)\n",
    "\n",
    "airport_cords = [\n",
    "    [40.691547, -74.180202], # NEWARK\n",
    "    [40.773281, -73.869845], # LAGUARDIA\n",
    "]\n",
    "\n",
    "# When I wrote this code, only God and I knew what was going on. Today, only God knows.\n",
    "def close_to_airport(values):\n",
    "    pickup_latitude = values['pickup_latitude']\n",
    "    pickup_longitude = values['pickup_longitude']\n",
    "    dropoff_latitude = values['dropoff_latitude']\n",
    "    dropoff_longitude = values['dropoff_longitude']\n",
    "\n",
    "    # JFK is special, it needs a bigger radius\n",
    "    jfkpickup = havardsine_distance(pickup_latitude, pickup_longitude, 40.645042, -73.786928) <= 1.00\n",
    "    jfkdropoff = havardsine_distance(dropoff_latitude, dropoff_longitude, 40.645042, -73.786928) <= 1.00\n",
    "\n",
    "    # Newark is a small airport but their parking log is BIG.\n",
    "    newarkpickup = havardsine_distance(pickup_latitude, pickup_longitude, 40.691547, -74.180202) <= 1.00\n",
    "    newarkdropoff = havardsine_distance(dropoff_latitude, dropoff_longitude, 40.691547, -74.180202) <= 1.00\n",
    "\n",
    "    # Third largest, it's an abstract airport with a stretched out U shaped parking place\n",
    "    # LaGuardia\n",
    "    laguardiapickup = havardsine_distance(pickup_latitude, pickup_longitude, 40.773855, -73.871712) <= 0.50\n",
    "    laguardiadropoff = havardsine_distance(dropoff_latitude, dropoff_longitude, 40.773855, -73.871712) <= 0.50\n",
    "\n",
    "    return 1 if jfkpickup or jfkdropoff or newarkpickup or newarkdropoff or laguardiapickup or laguardiadropoff else 0\n",
    "\n",
    "df['is_by_airport'] = df.apply(close_to_airport, axis=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "outputs": [],
   "source": [
    "my_time = df['pickup_datetime'][0]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "outputs": [],
   "source": [
    "df['EDTdate'] = df['pickup_datetime'] - pd.Timedelta(hours=4)\n",
    "df['Hour'] = df['EDTdate'].dt.hour\n",
    "df['AMorPM'] = np.where(df['Hour'] < 12, 0, 1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['04/19/10' '04/17/10' '04/11/10' '04/16/10' '04/22/10' '04/23/10'\n",
      " '04/15/10' '04/20/10' '04/21/10' '04/14/10' '04/13/10' '04/12/10'\n",
      " '04/24/10' '04/18/10']\n"
     ]
    }
   ],
   "source": [
    "df['Weekday'] = pd.to_numeric(df['EDTdate'].dt.strftime(\"%w\")) # an int between 0 - 6, representing the weekdays\n",
    "df['Month'] = pd.to_numeric(df['EDTdate'].dt.strftime(\"%m\")) # an int between 0 - 6, representing the weekdays\n",
    "\n",
    "#Formatting this correctly, we can use the dates to get the historical weather data for the dates\n",
    "df['FullDate'] = df['EDTdate'].dt.strftime('%x')\n",
    "#print(df['FullDate'].min())\n",
    "#print(df['FullDate'].max())\n",
    "print(df['FullDate'].unique())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "outputs": [
    {
     "data": {
      "text/plain": "'\\napi_key_openweather = \"661b8c5b6e790b9cf55403d0af1cebe8\"\\nnew_york_middle_lat = \"\"\\nnew_york_middle_long = \"\"\\nunique = np.sort(df[\\'FullDate\\'].unique())\\nstart = time.mktime(datetime.datetime.strptime(unique[0], \"%m/%d/%y\").timetuple())\\nend = time.mktime(datetime.datetime.strptime(unique[len(unique) - 1], \"%m/%d/%y\").timetuple())\\nurl = f\"https://history.openweathermap.org/data/2.5/history/city?lat={new_york_middle_lat}&lon={new_york_middle_long}&type=hour&start={start}&end={end}&appid={api_key_openweather}\"\\n\\nprint(url)\\n'"
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "api_key_openweather = \"661b8c5b6e790b9cf55403d0af1cebe8\"\n",
    "new_york_middle_lat = \"\"\n",
    "new_york_middle_long = \"\"\n",
    "unique = np.sort(df['FullDate'].unique())\n",
    "start = time.mktime(datetime.datetime.strptime(unique[0], \"%m/%d/%y\").timetuple())\n",
    "end = time.mktime(datetime.datetime.strptime(unique[len(unique) - 1], \"%m/%d/%y\").timetuple())\n",
    "url = f\"https://history.openweathermap.org/data/2.5/history/city?lat={new_york_middle_lat}&lon={new_york_middle_long}&type=hour&start={start}&end={end}&appid={api_key_openweather}\"\n",
    "\n",
    "print(url)\n",
    "'''\n",
    "# Historical weather data, not working"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "outputs": [],
   "source": [
    "# lets have some weather-y fuuuuuuuuuuuun-fuuuuuun-function\n",
    "# see this for doc https://www1.ncdc.noaa.gov/pub/data/cdo/documentation/GHCND_documentation.pdf\n",
    "weather_data = pd.read_csv('nyc-weather-data.csv')\n",
    "weather_labels = ['SNWD', 'SNOW', 'AWND'] #snow depth, did it snow\n",
    "for index, item in enumerate(weather_labels):\n",
    "    weather_data.loc[weather_data[item] <= -9999, item] = 0 # -9999 means that there is no data\n",
    "\n",
    "# df['dist_km'] = df.apply(lambda row: havardsine_distance(row.pickup_latitude, row.pickup_longitude, row.dropoff_latitude, row.dropoff_longitude), axis=1)\n",
    "\n",
    "weather_data['DATE'] = weather_data.apply(lambda row: datetime.datetime.strptime(str(row.DATE), '%Y%m%d').strftime('%m/%d/%y'), axis=1)\n",
    "weather_data = weather_data[['DATE','PRCP','SNWD','SNOW','TMAX','TMIN','AWND']]\n",
    "\n",
    "weather_data['TMIN'] = (weather_data['TMIN'] / 10) # the API returns degrees in an old format. We therefore convert the value to a tenth of its own form. Check the docu for this.\n",
    "weather_data['TMAX'] = (weather_data['TMAX'] / 10)\n",
    "weather_data['PRCP'] = weather_data['PRCP'] / 10\n",
    "\n",
    "weather_data['AWND'] = weather_data['AWND'] / 2.237 # meters per second\n",
    "\n",
    "df.rename(columns={'FullDate':'DATE'}, inplace=True)\n",
    "\n",
    "merged_df = pd.merge(df, weather_data, how=\"left\",on=\"DATE\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "outputs": [],
   "source": [
    "network_data = merged_df[['pickup_latitude', 'pickup_longitude', 'dropoff_latitude', 'dropoff_longitude', 'dist_km', 'passenger_count', 'is_by_airport', 'Hour', 'AMorPM', 'Weekday', 'Month', 'PRCP', 'SNWD', 'SNOW', 'TMAX', 'TMIN', 'AWND', 'fare_amount']]\n",
    "\n",
    "prediction = network_data['fare_amount']\n",
    "\n",
    "network_data.drop('fare_amount', axis=1)\n",
    "\n",
    "X = network_data.values\n",
    "\n",
    "y = prediction.values.reshape(-1,1)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "X_train = torch.FloatTensor(X_train)\n",
    "X_test = torch.FloatTensor(X_test)\n",
    "y_train = torch.FloatTensor(y_train)\n",
    "y_test = torch.FloatTensor(y_test)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "outputs": [],
   "source": [
    "class NYTaxiFareModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(in_features=18, out_features=20)\n",
    "        self.fc2 = nn.Linear(20, 20)\n",
    "        self.out = nn.Linear(20, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.out(x)\n",
    "        return x\n",
    "\n",
    "model = NYTaxiFareModel()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss()\n",
    "optim = torch.optim.Adam(model.parameters(), lr=0.001)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  1 loss:244.72637939\n",
      "epoch: 26 loss:244.72637939\n",
      "epoch: 51 loss:244.72637939\n",
      "epoch: 76 loss:244.72637939\n",
      "epoch: 101 loss:244.72637939\n",
      "epoch: 126 loss:244.72637939\n",
      "epoch: 151 loss:244.72637939\n",
      "epoch: 176 loss:244.72637939\n",
      "epoch: 201 loss:244.72637939\n",
      "epoch: 226 loss:244.72637939\n",
      "epoch: 251 loss:244.72637939\n",
      "epoch: 276 loss:244.72637939\n",
      "epoch: 301 loss:244.72637939\n",
      "epoch: 326 loss:244.72637939\n",
      "epoch: 351 loss:244.72637939\n",
      "epoch: 376 loss:244.72637939\n",
      "epoch: 401 loss:244.72637939\n",
      "epoch: 426 loss:244.72637939\n",
      "epoch: 451 loss:244.72637939\n",
      "epoch: 476 loss:244.72637939\n"
     ]
    }
   ],
   "source": [
    "epochs = 500\n",
    "losses = []\n",
    "\n",
    "for i in range(epochs):\n",
    "    y_pred = model(X_train)\n",
    "    loss = criterion(y_pred, y_train)\n",
    "    losses.append(loss)\n",
    "\n",
    "    if i % 25 == 1:\n",
    "        print(f'epoch: {i:2} loss:{loss.item():10.8f}')\n",
    "    optim.zero_grad()\n",
    "    loss.backward()\n",
    "    optim.step()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}